{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"A nation can progress and attain higher development growth only when both men and women are entitled to equal opportunities. Women in the society are often cornered and are refrained from getting equal rights as men to health, education, decision-making and economic independence in terms of wages.\n",
    "\n",
    "The social structure that prevails since long in such a way that girls do not get equal opportunities as men. Women generally are the caregivers in the family. Because of this, women are mostly involved in household activities. There is lesser participation of women in higher education, decision-making roles, and leadership roles. This gender disparity is a hindrance in the growth rate of a country. When women participate in the workforce increases the economic growth rate of the country increases. Gender equality increases the overall wellbeing of the nation along with\n",
    "Gender equality is an important factor in determining a country’s overall growth. There are several indexes to measure gender equality.\n",
    "\n",
    "Gender-Related Development Index (GDI)    GDI is a gender centric measure of Human Development Index. GDI considers parameters like life expectancy, education, and incomes in assessing the gender equality of a country.\n",
    "\n",
    "Gender Empowerment Measure (GEM)  This measure includes much detail aspects like the proportion of seats than women candidates hold in national parliament, percentage of women at economic decision-making role, the income share of female employees.\n",
    "\n",
    "Gender Equity Index (GEI)  GEI ranks countries on three parameters of gender inequality, those are education, economic participation, and empowerment. However, GEI ignores the health parameter.\n",
    "\n",
    "Global Gender Gap Index  The World Economic Forum introduced the Global Gender Gap Index in 2006. This index focuses more on identifying the level of female disadvantage. The four important areas that the index considers are economic participation and opportunity, educational attainment, political empowerment, health, and survival rate.\n",
    "As per the World Economic Forum’s gender gap ranking, India stands at rank 108 out of 149 countries. This rank is a major concern as it highlights the immense gap in opportunities in women with comparison to men. In Indian society from a long time back, the social structure has been such that the women are neglected in many areas like education, health, decision-making areas, financial independence, etc.\n",
    "\n",
    "You can check the information on- https://www.youtube.com/watch?v=_04qU5kbxjg\n",
    "\n",
    "We are so grateful to have here- https://www.youtube.com/watch?v=qMJQ6JVkjeM\n",
    "\n",
    "Another major reason, which contributes to the discriminatory behavior towards women in India, is the dowry system in marriage.  Because of this dowry system, most Indian families consider girls as a burden.  Preference for son still prevails. Girls have refrained from higher education. Women are not entitled to equal job opportunities and wages. In the 21st century, women are still preferred gender in home managing activities. Many women quit their job and opt-out from leadership roles because of family commitments. However, such actions are very uncommon among men.\n",
    "For overall wellbeing and growth of a nation, scoring high on gender equality is the most crucial aspect. Countries with less disparity in gender equality have progressed a lot. The government of India has also started taking steps to ensure gender equality. Several laws and policies are prepared to encourage girls. “Beti Bachao, Beti Padhao Yojana” (Save girl, and make girls educated) campaign is created to spread awareness of the importance of girl child.  Several laws to protect girls are also there. However, we need more awareness of spreading knowledge of women rights. In addition, the government should take initiatives to check the correct and proper implementation of policies.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove superscript texts\n",
    "import unicodedata\n",
    "text = \"\".join(c for c in text if unicodedata.category(c) not in [\"No\", \"Lo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove puctuation errors\n",
    "import string\n",
    "for punctuation in string.punctuation:\n",
    "    text = text.replace(punctuation, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to convert all the values to lower case\n",
    "text=text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to remove all the URLs\n",
    "import re\n",
    "hello = re.sub(r'http\\S+', '', text, flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a nation can progress and attain higher development growth only when both men and women are entitled to equal opportunities women in the society are often cornered and are refrained from getting equal rights as men to health education decisionmaking and economic independence in terms of wages\n",
      "\n",
      "the social structure that prevails since long in such a way that girls do not get equal opportunities as men women generally are the caregivers in the family because of this women are mostly involved in household activities there is lesser participation of women in higher education decisionmaking roles and leadership roles this gender disparity is a hindrance in the growth rate of a country when women participate in the workforce increases the economic growth rate of the country increases gender equality increases the overall wellbeing of the nation along with\n",
      "gender equality is an important factor in determining a country’s overall growth there are several indexes to measure gender equality\n",
      "\n",
      "genderrelated development index gdi    gdi is a gender centric measure of human development index gdi considers parameters like life expectancy education and incomes in assessing the gender equality of a country\n",
      "\n",
      "gender empowerment measure gem  this measure includes much detail aspects like the proportion of seats than women candidates hold in national parliament percentage of women at economic decisionmaking role the income share of female employees\n",
      "\n",
      "gender equity index gei  gei ranks countries on three parameters of gender inequality those are education economic participation and empowerment however gei ignores the health parameter\n",
      "\n",
      "global gender gap index  the world economic forum introduced the global gender gap index in 2006 this index focuses more on identifying the level of female disadvantage the four important areas that the index considers are economic participation and opportunity educational attainment political empowerment health and survival rate\n",
      "as per the world economic forum’s gender gap ranking india stands at rank 108 out of 149 countries this rank is a major concern as it highlights the immense gap in opportunities in women with comparison to men in indian society from a long time back the social structure has been such that the women are neglected in many areas like education health decisionmaking areas financial independence etc\n",
      "\n",
      "you can check the information on httpswwwyoutubecomwatchv04qu5kbxjg\n",
      "\n",
      "we are so grateful to have here httpswwwyoutubecomwatchvqmjq6jvkjem\n",
      "\n",
      "another major reason which contributes to the discriminatory behavior towards women in india is the dowry system in marriage  because of this dowry system most indian families consider girls as a burden  preference for son still prevails girls have refrained from higher education women are not entitled to equal job opportunities and wages in the 21st century women are still preferred gender in home managing activities many women quit their job and optout from leadership roles because of family commitments however such actions are very uncommon among men\n",
      "for overall wellbeing and growth of a nation scoring high on gender equality is the most crucial aspect countries with less disparity in gender equality have progressed a lot the government of india has also started taking steps to ensure gender equality several laws and policies are prepared to encourage girls “beti bachao beti padhao yojana” save girl and make girls educated campaign is created to spread awareness of the importance of girl child  several laws to protect girls are also there however we need more awareness of spreading knowledge of women rights in addition the government should take initiatives to check the correct and proper implementation of policies\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytokenizer = Tokenizer()\n",
    "mytokenizer.fit_on_texts([text])\n",
    "total_words = len(mytokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mytokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for sentence in text.split('\\n'):\n",
    "  tokenized_sentence = mytokenizer.texts_to_sequences([sentence])[0]\n",
    "\n",
    "  for i in range(1,len(tokenized_sentence)):\n",
    "    input_sequences.append(tokenized_sentence[:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  8, 29, 52])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  8, 29])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 29,  52,  97,   5,  98,  30,  31,  17,  99,  53, 100,  18,   5,\n",
       "         6,   7,  54,   9,  20,  21,   6,   3,   1,  55,   7, 101, 102,\n",
       "         5,   7,  56,  22, 103,  20,  57,  19,  18,   9,  23,  14,  24,\n",
       "         5,  11,  58,   3, 104,   2,  59,  60,  61,  25,  62, 105,  63,\n",
       "         3,  32,   8, 106,  25,  15, 107,  64, 108,  20,  21,  19,  18,\n",
       "         6, 109,   7,   1, 110,   3,   1,  65,  33,   2,  16,   6,   7,\n",
       "       111, 112,   3, 113,  66,  34,  10, 114,  35,   2,   6,   3,  30,\n",
       "        14,  24,  36,   5,  67,  36,  16,   4,  68,  10,   8, 115,   3,\n",
       "         1,  17,  37,   2,   8,  38,  53,   6, 116,   3,   1, 117,  39,\n",
       "         1,  11,  17,  37,   2,   1,  38,  39,   4,  12,  39,   1,  40,\n",
       "        69,   2,   1,  29, 118,  41,  12,  10, 119,  70, 120,   3, 121,\n",
       "         8, 122,  40,  17,  34,   7,  42, 123,   9,  26,   4,  12,  31,\n",
       "        13,  43,  43,  10,   8,   4, 125,  26,   2, 126,  31,  13,  43,\n",
       "        71,  72,  44, 127, 128,  14,   5, 129,   3, 130,   1,   4,  12,\n",
       "         2,   8,  38,  45,  26, 131,  16,  26, 132, 133, 134, 135,  44,\n",
       "         1, 136,   2, 137, 138,   6, 139, 140,   3, 141, 142, 143,   2,\n",
       "         6,  73,  11,  24, 144,   1, 145, 146,   2,  74, 147, 148,  13,\n",
       "        46,  46, 149,  47,  27, 150,  72,   2,   4, 151, 152,   7,  14,\n",
       "        11,  35,   5,  45,  48,  46, 153,   1,  23, 154,   4,  28,  13,\n",
       "         1,  76,  11, 155, 156,   1,  75,   4,  28,  13,   3, 157,  16,\n",
       "        13, 158,  77,  27, 159,   1, 160,   2,  74, 161,   1, 162,  70,\n",
       "        49,  25,   1,  13,  71,   7,  11,  35,   5, 163, 164, 165, 166,\n",
       "        45,  23,   5, 167,  37, 168,   1,  76,  11, 169,   4,  28, 170,\n",
       "        50, 171,  73,  78, 172, 173,   2, 174,  47,  16,  78,  10,   8,\n",
       "        79, 175,  19, 176, 177,   1, 178,  28,   3,  21,   3,   6,  41,\n",
       "       179,   9,  18,   3,  80,  55,  22,   8,  63, 180, 181,   1,  60,\n",
       "        61,  81, 182,  32,  25,   1,   6,   7, 183,   3,  82,  49,  44,\n",
       "        14,  23,  24,  49, 184,  58, 185,  52,  83,   1, 187,  27, 188,\n",
       "         7, 189, 190,   9,  51, 191, 192,  79, 194, 195, 196,   9,   1,\n",
       "       197, 198, 199,   6,   3,  50,  10,   1,  85,  86,   3, 200,  33,\n",
       "         2,  16,  85,  86,  87,  80, 201, 202,  15,  19,   8, 203, 204,\n",
       "        88, 205,  89,  62,  15,  51,  56,  22,  30,  14,   6,   7,  64,\n",
       "        54,   9,  20,  90,  21,   5,  59,   3,   1, 206, 207,   6,   7,\n",
       "        89, 208,   4,   3, 209, 210,  66,  82,   6, 211, 212,  90,   5,\n",
       "       213,  22,  67,  36,  33,   2,  65, 214,  48,  32, 215,   7, 216,\n",
       "       217, 218,  18,  40,  69,   5,  17,   2,   8,  29, 219, 220,  27,\n",
       "         4,  12,  10,   1,  87, 221, 222,  47,  41, 223,  68,   3,   4,\n",
       "        12,  51, 224,   8, 225,   1,  91,   2,  50,  81,  92, 226, 227,\n",
       "       228,   9, 229,   4,  12,  42,  93,   5,  94,   7, 230,   9, 231,\n",
       "        15, 232, 233, 234, 235, 236, 237,  95,   5, 238,  15, 239, 240,\n",
       "        10, 241,   9, 242,  96,   2,   1, 243,   2,  95, 244,  42,  93,\n",
       "         9, 245,  15,   7,  92,  34,  48,  84, 246,  77,  96,   2, 247,\n",
       "       248,   2,   6,  57,   3, 249,   1,  91, 250, 251, 252,   9,  83,\n",
       "         1, 253,   5, 254, 255,   2,  94])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0.], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 108, 100)          25600     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 150)               150600    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               38656     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 214,856\n",
      "Trainable params: 214,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18/18 [==============================] - 10s 126ms/step - loss: 5.5229 - accuracy: 0.0353\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 2s 129ms/step - loss: 5.2499 - accuracy: 0.0477\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 5.0815 - accuracy: 0.0548\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 5.0469 - accuracy: 0.0548\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 2s 125ms/step - loss: 5.0356 - accuracy: 0.0548\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 2s 126ms/step - loss: 5.0146 - accuracy: 0.0548\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 2s 125ms/step - loss: 4.9690 - accuracy: 0.0671\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 2s 126ms/step - loss: 4.9133 - accuracy: 0.0830\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 2s 123ms/step - loss: 4.8439 - accuracy: 0.0742\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 2s 119ms/step - loss: 4.7460 - accuracy: 0.0724\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 2s 119ms/step - loss: 4.6394 - accuracy: 0.1007\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 2s 118ms/step - loss: 4.5138 - accuracy: 0.1095\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 2s 120ms/step - loss: 4.3925 - accuracy: 0.1272\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 2s 123ms/step - loss: 4.2230 - accuracy: 0.1413\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 2s 121ms/step - loss: 4.0383 - accuracy: 0.1519\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 3.8408 - accuracy: 0.1661\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 2s 116ms/step - loss: 3.6589 - accuracy: 0.1943\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 2s 115ms/step - loss: 3.4766 - accuracy: 0.2049\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 2s 116ms/step - loss: 3.2720 - accuracy: 0.2314\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 2s 114ms/step - loss: 3.0960 - accuracy: 0.2633\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 2.9106 - accuracy: 0.3233\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 2s 115ms/step - loss: 2.7217 - accuracy: 0.3657\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 2.5453 - accuracy: 0.4134\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 2.3827 - accuracy: 0.4611\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 2.2274 - accuracy: 0.5000\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 2.0833 - accuracy: 0.5618\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 2s 110ms/step - loss: 1.9449 - accuracy: 0.6025\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 2s 110ms/step - loss: 1.8068 - accuracy: 0.6625\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 2s 110ms/step - loss: 1.6709 - accuracy: 0.7120\n",
      "Epoch 30/100\n",
      "18/18 [==============================] - 2s 115ms/step - loss: 1.5573 - accuracy: 0.7509\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 2s 112ms/step - loss: 1.4472 - accuracy: 0.8163\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 3s 148ms/step - loss: 1.3426 - accuracy: 0.8604\n",
      "Epoch 33/100\n",
      "18/18 [==============================] - 2s 121ms/step - loss: 1.2449 - accuracy: 0.8799\n",
      "Epoch 34/100\n",
      "18/18 [==============================] - 2s 120ms/step - loss: 1.1494 - accuracy: 0.9117\n",
      "Epoch 35/100\n",
      "18/18 [==============================] - 2s 117ms/step - loss: 1.0623 - accuracy: 0.9329\n",
      "Epoch 36/100\n",
      "18/18 [==============================] - 2s 116ms/step - loss: 0.9774 - accuracy: 0.9470\n",
      "Epoch 37/100\n",
      "18/18 [==============================] - 2s 119ms/step - loss: 0.9074 - accuracy: 0.9611\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - 2s 120ms/step - loss: 0.8439 - accuracy: 0.9611\n",
      "Epoch 39/100\n",
      "18/18 [==============================] - 2s 122ms/step - loss: 0.7796 - accuracy: 0.9770\n",
      "Epoch 40/100\n",
      "18/18 [==============================] - 2s 129ms/step - loss: 0.7266 - accuracy: 0.9841\n",
      "Epoch 41/100\n",
      "18/18 [==============================] - 2s 120ms/step - loss: 0.6745 - accuracy: 0.9912\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - 2s 115ms/step - loss: 0.6236 - accuracy: 0.9912\n",
      "Epoch 43/100\n",
      "18/18 [==============================] - 2s 119ms/step - loss: 0.5841 - accuracy: 0.9929\n",
      "Epoch 44/100\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.5461 - accuracy: 0.9947\n",
      "Epoch 45/100\n",
      "18/18 [==============================] - 2s 125ms/step - loss: 0.5054 - accuracy: 0.9947\n",
      "Epoch 46/100\n",
      "18/18 [==============================] - 2s 120ms/step - loss: 0.4759 - accuracy: 0.9947\n",
      "Epoch 47/100\n",
      "18/18 [==============================] - 2s 118ms/step - loss: 0.4443 - accuracy: 0.9947\n",
      "Epoch 48/100\n",
      "18/18 [==============================] - 2s 118ms/step - loss: 0.4173 - accuracy: 0.9947\n",
      "Epoch 49/100\n",
      "18/18 [==============================] - 2s 116ms/step - loss: 0.3947 - accuracy: 0.9965\n",
      "Epoch 50/100\n",
      "18/18 [==============================] - 2s 124ms/step - loss: 0.3717 - accuracy: 0.9965\n",
      "Epoch 51/100\n",
      "18/18 [==============================] - 2s 117ms/step - loss: 0.3481 - accuracy: 0.9965\n",
      "Epoch 52/100\n",
      "18/18 [==============================] - 2s 116ms/step - loss: 0.3283 - accuracy: 0.9965\n",
      "Epoch 53/100\n",
      "18/18 [==============================] - 2s 112ms/step - loss: 0.3083 - accuracy: 0.9947\n",
      "Epoch 54/100\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.2929 - accuracy: 0.9947\n",
      "Epoch 55/100\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.2767 - accuracy: 0.9947\n",
      "Epoch 56/100\n",
      "18/18 [==============================] - 2s 114ms/step - loss: 0.2636 - accuracy: 0.9947\n",
      "Epoch 57/100\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.2501 - accuracy: 0.9965\n",
      "Epoch 58/100\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.2372 - accuracy: 0.9965\n",
      "Epoch 59/100\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.2259 - accuracy: 0.9965\n",
      "Epoch 60/100\n",
      "18/18 [==============================] - 2s 112ms/step - loss: 0.2163 - accuracy: 0.9965\n",
      "Epoch 61/100\n",
      "18/18 [==============================] - 2s 112ms/step - loss: 0.2056 - accuracy: 0.9965\n",
      "Epoch 62/100\n",
      "18/18 [==============================] - 2s 114ms/step - loss: 0.1959 - accuracy: 0.9947\n",
      "Epoch 63/100\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.1866 - accuracy: 0.9965\n",
      "Epoch 64/100\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.1790 - accuracy: 0.9965\n",
      "Epoch 65/100\n",
      "18/18 [==============================] - 2s 114ms/step - loss: 0.1716 - accuracy: 0.9947\n",
      "Epoch 66/100\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.1647 - accuracy: 0.9947\n",
      "Epoch 67/100\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.1579 - accuracy: 0.9965\n",
      "Epoch 68/100\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.1514 - accuracy: 0.9965\n",
      "Epoch 69/100\n",
      "18/18 [==============================] - 2s 117ms/step - loss: 0.1462 - accuracy: 0.9947\n",
      "Epoch 70/100\n",
      "18/18 [==============================] - 2s 112ms/step - loss: 0.1405 - accuracy: 0.9947\n",
      "Epoch 71/100\n",
      "18/18 [==============================] - 2s 115ms/step - loss: 0.1353 - accuracy: 0.9965\n",
      "Epoch 72/100\n",
      "18/18 [==============================] - 2s 112ms/step - loss: 0.1296 - accuracy: 0.9947\n",
      "Epoch 73/100\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.1252 - accuracy: 0.9947\n",
      "Epoch 74/100\n",
      "18/18 [==============================] - 2s 114ms/step - loss: 0.1209 - accuracy: 0.9947\n",
      "Epoch 75/100\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.1167 - accuracy: 0.9947\n",
      "Epoch 76/100\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.1126 - accuracy: 0.9947\n",
      "Epoch 77/100\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.1090 - accuracy: 0.9965\n",
      "Epoch 78/100\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.1058 - accuracy: 0.9947\n",
      "Epoch 79/100\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.1030 - accuracy: 0.9947\n",
      "Epoch 80/100\n",
      "18/18 [==============================] - 2s 110ms/step - loss: 0.0990 - accuracy: 0.9965\n",
      "Epoch 81/100\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.0960 - accuracy: 0.9947\n",
      "Epoch 82/100\n",
      "18/18 [==============================] - 2s 112ms/step - loss: 0.0932 - accuracy: 0.9947\n",
      "Epoch 83/100\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.0901 - accuracy: 0.9965\n",
      "Epoch 84/100\n",
      "18/18 [==============================] - 2s 117ms/step - loss: 0.0880 - accuracy: 0.9947\n",
      "Epoch 85/100\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.0852 - accuracy: 0.9947\n",
      "Epoch 86/100\n",
      "18/18 [==============================] - 2s 113ms/step - loss: 0.0821 - accuracy: 0.9965\n",
      "Epoch 87/100\n",
      "18/18 [==============================] - 2s 110ms/step - loss: 0.0798 - accuracy: 0.9965\n",
      "Epoch 88/100\n",
      "18/18 [==============================] - 2s 112ms/step - loss: 0.0776 - accuracy: 0.9965\n",
      "Epoch 89/100\n",
      "18/18 [==============================] - 2s 115ms/step - loss: 0.0757 - accuracy: 0.9947\n",
      "Epoch 90/100\n",
      "18/18 [==============================] - 2s 117ms/step - loss: 0.0736 - accuracy: 0.9947\n",
      "Epoch 91/100\n",
      "18/18 [==============================] - 2s 112ms/step - loss: 0.0721 - accuracy: 0.9965\n",
      "Epoch 92/100\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.0706 - accuracy: 0.9947\n",
      "Epoch 93/100\n",
      "18/18 [==============================] - 2s 112ms/step - loss: 0.0682 - accuracy: 0.9965\n",
      "Epoch 94/100\n",
      "18/18 [==============================] - 2s 112ms/step - loss: 0.0665 - accuracy: 0.9965\n",
      "Epoch 95/100\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.0650 - accuracy: 0.9965\n",
      "Epoch 96/100\n",
      "18/18 [==============================] - 2s 112ms/step - loss: 0.0633 - accuracy: 0.9965\n",
      "Epoch 97/100\n",
      "18/18 [==============================] - 2s 110ms/step - loss: 0.0616 - accuracy: 0.9947\n",
      "Epoch 98/100\n",
      "18/18 [==============================] - 2s 111ms/step - loss: 0.0606 - accuracy: 0.9965\n",
      "Epoch 99/100\n",
      "18/18 [==============================] - 2s 112ms/step - loss: 0.0590 - accuracy: 0.9965\n",
      "Epoch 100/100\n",
      "18/18 [==============================] - 2s 112ms/step - loss: 0.0579 - accuracy: 0.9965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fde059ff40>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[193, 79, 194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 787ms/step\n",
      "[193, 79, 194, 195]\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "[193, 79, 194, 195, 196]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "[193, 79, 194, 195, 196, 9]\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "[193, 79, 194, 195, 196, 9, 1]\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "[193, 79, 194, 195, 196, 9, 1, 197]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "another major reason which contributes to the discriminatory behavior\n"
     ]
    }
   ],
   "source": [
    "input_text = \"another major reason\"\n",
    "predict_next_words= 6\n",
    "\n",
    "for _ in range(predict_next_words):\n",
    "    token_list = mytokenizer.texts_to_sequences([input_text])[0]\n",
    "    print(token_list)\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in mytokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    input_text += \" \" + output_word\n",
    "\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.054992325603961945\n",
      "Test accuracy: 0.9964664578437805\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X,y, verbose = 0) \n",
    "\n",
    "print('Test loss:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vedant_envir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
